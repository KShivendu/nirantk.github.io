Topics marked with * imply that I have not tried that myself, so I am even more eager to see what you come up with
Reference Work gives you the minimum possible quality of output. You are expected to do better than this.  

| When   | Topic                                               | Study Links                                                                                                                                                                                                                                                              | Deliverable                                                                                                                                                                    | Estimated Time                                                                                    | Reference Work                                                                                                                                                                                                                                                                               | Submission                                                                 |
|--------|-----------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|
| Week 1 | LM Pretraining and Sub-word Models                  | cs224n slides;BERT Pytorch implementation - Check the tokenizer; Mikolov paper; Words vs Morpheme: PaperStemming vs lemmatization (IR-Book) Zipf’s Law: https://en.wikipedia.org/wiki/Zipf%27s_law                                                                       | Notebook and Model: Sub-word text classification model on a standard dataset Compare against a word tokenizerLink to standard classification datasets                          | 10-12 hours                                                                                       | Subword Tokenizer: IMDB https://github.com/n-waves/multifit                                                                                                                                                                                                                                  |                                                                            |
| Week 2 | Multilingual Models                                 | ULMFit for Non English Languages,iNLTK, Indic NLP Library                                                                                                                                                                                                                | Train one single model of MultiFit-QRNN on the following languages: Hindi, Tamil, Telugu, Malayalam, Bengali, Arabic, Urdu, NepaliRelease Notebook and Pretrained LM on Github | 10-15 hours                                                                                       |                                                                                                                                                                                                                                                                                              |                                                                            |
| Week 3 | Sequence Tagging; NER Basics                        | PyTorch BiLSTM-CRF Tutorial; Flair; Evaluation Metrics (Free Registration) Haptik Chatbot NER;                                                                                                                                                                           | Notebook: Implement a Neural text augmentation strategy e.g. back-translation and use with flair for NERSimpler: Use Existing transforms from textacy                          | 10-20 hours Hint: Can you extend Delete, Retrieve, Generate to work off a fastai language model?* | Reference Work: Yandex NER Homework TextAugment for Short Text Classification                                                                                                                                                                                                                | https://colab.research.google.com/drive/10RuTE8uVq2RQdUgqJMaeQMmB1O5af4fQ  |
| Week 4 | ~No New Topics~                                     | Yes, you should write a blog - Rachel Thoma                                                                                                                                                                                                                              | Blog or new experiment notebook: Based on any 2 or more themes                                                                                                                 | 3-7 hours                                                                                         | Blog : Arctic Monkeys Lyrics Generator                                                                                                                                                                                                                                                       |                                                                            |
| Week 5 | Transfer Learning +Seq2Seq                          | Yandex Course; NLP ImageNet Moment; Illustrated BERT; fastai NLP; Optional, but Recommended: Domain Adaptation Sections 3.4 and 4 from Ruder’s Thesis; Section 7 on fine-tuning pretrained modelsPyTorch Transformers; Sentence-Transformers  spaCy-PyTorch Transformers | Notebook:  Try out BART (not BERT)Write a tutorial on how to use this for Summarization* Bonus: T5 from Google on Summarization                                                | 6-10 hours                                                                                        | Notebook: Try out BERT on an email classification problem. BERT for MailBlog: Write a 200-500 word blog on Transfer LearningBlog Post Ideas:ConvAI Transformer language modelHuggingFace ConvAI2Word2Vec for Emojis using Twitter DataGenerate Text captions for Gifs using a Twitter Corpus |                                                                            |
| Week 6 | Prompt Design with GPT2/T5; NER/Span Categorisation | Flair; Haptik Chatbot NER; TBD                                                                                                                                                                                                                                           | Notebook TBD Optional: Tutorial implementing NER for a chat corpus with a list of custom entities                                                                              | 5-7 hours                                                                                         | TBD                                                                                                                                                                                                                                                                                          |                                                                            |
| Week 7 | Neural Search, CLIP + VQGAN                         | Guest Lecture by Pratik Bhavsar                                                                                                                                                                                                                                          |                                                                                                                                                                                |                                                                                                   |                                                                                                                                                                                                                                                                                              |                                                                            |
| Week 8 | Model Interpretations and Visualisation             | Kaggle Learn module on the topic; example: BERT Multi-lingual Vocabulary; GPT-2 demo                                                                                                                                                                                     | Notebook or blog: Explain BERT and USE predictions with one or more methods                                                                                                    | 6-10 hours                                                                                        | Model Interpretation                                                                                                                                                                                                                                                                         |                                                                            |
| Week 9 | ~No New Topics~                                     |                                                                                                                                                                                                                                                                          |                                                                                                                                                                                |                                                                                                   |                                                                                                                                                                                                                                                                                              |                                                                            |