{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../20230505_Messages.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc49eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Message\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a210b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by week\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
    "df['Week'] = df['Datetime'].dt.isocalendar().week\n",
    "df['Date'] = df['Datetime'].dt.date\n",
    "\n",
    "# Group by Date\n",
    "daily_df = df.groupby('Date').agg({'Message': ' \\n '.join}).reset_index()\n",
    "daily_df = pd.DataFrame(daily_df)\n",
    "len(daily_df)\n",
    "\n",
    "# # Group by Week\n",
    "# weekly_df = df.groupby('Week').agg({'Message': ' \\n '.join}).reset_index()\n",
    "# weekly_df = pd.DataFrame(messages_df)\n",
    "# print(weekly)\n",
    "# print(weekly_df[\"Message\"][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[\"wc\"] = daily_df[\"Message\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8296bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[\"wc\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77948434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from functools import lru_cache\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8699fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "class SummarizeDay:\n",
    "    def __init__(self, plain_text):\n",
    "        self.prompt_template = \"\"\"This is a chaotic Generative AI Group Chat transcript. Write detailed, exhaustive bullet point recap of topics discussed. Extract COMPLETE URL of web and social links with context. Please organise it into sections, only when needed:\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "Use Markdown. Add ## for section titles. TOPICS RECAP:\"\"\"\n",
    "# Research with weblinks where relevant EXACTLY ONCE:\n",
    "        self.PROMPT = PromptTemplate(template=self.prompt_template, input_variables=[\"text\"])\n",
    "#         self.chain = load_summarize_chain(ChatOpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "        self.chain = load_summarize_chain(ChatOpenAI(temperature=0), chain_type=\"stuff\", prompt=self.PROMPT)\n",
    "        self.docs = self.make_docs(plain_text)\n",
    "        \n",
    "    @lru_cache\n",
    "    def make_docs(self, plain_text: str):\n",
    "        texts = text_splitter.split_text(plain_text)\n",
    "        docs = [Document(page_content=t) for t in texts]\n",
    "        return docs\n",
    "\n",
    "    def summarize_docs(self):   \n",
    "        chain_output = self.chain({\"input_documents\": self.docs}, return_only_outputs=True)\n",
    "        return chain_output\n",
    "\n",
    "plain_text = daily_df[\"Message\"][36]\n",
    "sd = SummarizeDay(plain_text)\n",
    "chain_output = sd.summarize_docs()\n",
    "output_text = chain_output[\"output_text\"]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "WINDOW = 1\n",
    "def extract_urls_with_context(text):\n",
    "    lines = text.split('\\n')\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    urls_with_context = []\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        for match in url_pattern.finditer(line):\n",
    "            start, end = match.span()\n",
    "            prev_line = lines[idx - WINDOW] if idx > 0 else \"\"\n",
    "            next_line = lines[idx + WINDOW] if idx < len(lines) - 1 else \"\"\n",
    "            context = f\"{prev_line}\\n{line}\\n{next_line}\".strip()\n",
    "            urls_with_context.append((match.group(), context))\n",
    "\n",
    "    return urls_with_context\n",
    "\n",
    "\n",
    "    urls_with_context = extract_urls_with_context(url)\n",
    "    for url, context in urls_with_context:\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Context: {context}\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "class LinksContext:\n",
    "    def __init__(self, plain_text):\n",
    "        self.prompt_template = \"\"\"For each URL, there is some context. Newlines may or may not be related to the link, but the message in the same link as link is related to the link.\n",
    "        \n",
    "{text}\n",
    "        \n",
    "Mention URL with context. Single bullet point:\"\"\"\n",
    "# Research with weblinks where relevant EXACTLY ONCE:\n",
    "        self.PROMPT = PromptTemplate(template=self.prompt_template, input_variables=[\"text\"])\n",
    "#         self.chain = load_summarize_chain(ChatOpenAI(temperature=0, model_name=\"gpt-4\"), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "        self.chain = load_summarize_chain(ChatOpenAI(temperature=0), chain_type=\"stuff\", prompt=self.PROMPT)\n",
    "        self.docs = self.make_docs(plain_text)\n",
    "        \n",
    "    @lru_cache\n",
    "    def make_docs(self, plain_text: str):\n",
    "        texts = text_splitter.split_text(plain_text)\n",
    "        docs = [Document(page_content=t) for t in texts]\n",
    "        return docs\n",
    "\n",
    "    def summarize_docs(self):   \n",
    "        chain_output = self.chain({\"input_documents\": self.docs}, return_only_outputs=True)\n",
    "        return chain_output\n",
    "\n",
    "plain_text = daily_df[\"Message\"][54]\n",
    "url_groups = extract_urls_with_context(plain_text)\n",
    "for ug in url_groups:\n",
    "    lc = LinksContext(ug[1])\n",
    "    chain_output = lc.summarize_docs()\n",
    "    output_text = chain_output[\"output_text\"]\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13beefbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
